{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2YtDybiK4GC6xftwYl7rf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3mzPAUKm27y9"},"source":["\n","#Downloading gpt2 weights and applying PEFT with loRA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0p6f3ynFzMzq"},"outputs":[],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","# Initialize the GPT-2 tokenizer using the pre-trained \"gpt2\" version\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Initialize the GPT-2 model using the pre-trained \"gpt2\" version\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6ZtLMGHvzVJu"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n","for param in model.parameters():\n","  param.requires_grad = False  # freeze the model - train adapters later\n","  if param.ndim == 1:\n","    # cast the small parameters (e.g. layernorm) to fp32 for stability\n","    param.data = param.data.to(torch.float32)\n","\n","model.gradient_checkpointing_enable()  # reduce number of stored activations\n","model.enable_input_require_grads()\n","\n","class CastOutputToFloat(nn.Sequential):\n","  def forward(self, x): return super().forward(x).to(torch.float32)\n","model.lm_head = CastOutputToFloat(model.lm_head)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QR0_OeHa0AAO"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oGausNX90EOO"},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=8, #attention heads\n","    lora_alpha=32, #alpha scaling\n","    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{"id":"g6mCnnp13XNa"},"source":["#Tokenizing data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273,"referenced_widgets":["9c2d3669a5994af09b205dafaee9aecf","9500a3646c344870899160436008ec7f","bc85abc78f45475a96efe472e77e8990","e554f5b59eee42d59c4f251d638f3b62","b98e911b782a4543b870e4a31fbf6c6b","6a1ab538c0eb4b7495dbe1f692d9bf09","ada31c4f603d40e1a7d91b7651e0c92c","fca80b0a26cb4d86996b080804c3abdd","3205e91aa31041a39edfc2bf43c1fa6d","9cdd73ac80eb4226ac6299789e09d365","040d6a31ceb04c7fb773a5d23e8adcf9","fc23c68e5a2449af9da6af816694ebb2","3fc090c144514c12aea291bceeeaa9ac","6e7ccd8ad30c450cbe83082fc62fd5eb","2400b097eea74ba3aa65f241e59ae23b","af2fa5f6ddc24c17acd75d8d809d5d3c","6900cfe70d994ed5b05f3177c8167a6c","62501122d58a47d18b6ced71573032f5","191450a542284e18b61cd878acca179a","2b2045b58c1f4d8ebd26623c7349df1c","67485dd7f11f4dc58422d63d00b9980f","a8a1c713ceba4cca9fc01995eb2be294","9313de894468429da6b9bca9e19db9af","480945a86cad4687981dd6502027cc2e","018751c1552646c396efa03a0dcb7df4","6266a3088bfd4a7b88c60360ab5bd63d","69c52522f26342609f661cf0cfee5d8e","93ff4cb150684caea72dbb9d9c4431b0","aa15113c84df4184b737028755d8c8d8","007531065aec46fab027945b4fd374fb","4d18edc059f6402dacb1fe4115793e30","73ea01ffd43f45f8855aa115c2eccf17","6f68a3e36d92412f8e6649aef45650e1","6deda7d375ac4ee689cc26ce9fbb67d4","6c1798e63e254e81a25b12b73c323b0b","a25ca8c5bf544e79a9af0235681e6753","418788b476624c138879e5b308be37a1","b2fce5d9da8a419f9b0b6b782d8e323b","f21257a37bc14ac6970fbc4922f56675","07acd0a5146f49d5a126952783ac6346","58a5f632613346cba8b5b6423187f0af","de5586249d54428eae0724873a72d6a5","98c64286df1740c4b4dd0ff82d55664c","37a5e1dc98f24b93b7b4d64844eff4cb","45295bce67c141d4b40cec6d07a0890c","16cd2839c0bd46919e2b6c4537c069c5","ffc99956030448768956977c54c93005","84bac50a943849c0a33ef8d97f8278c5","2ea3fda98b4442c997ad77444b5a668c","b7a19a7cf340409fa255538e3864cfc1","b295689554e34df08d41745bd5defac3","0365a8db237f497a9153bd21efe606f9","4bfbfef74bf94bdd9ecf855f0b4b5502","86096d1802984af0b4cf4bc536c7a9d4","0c74354ef9da487fbe341cffb9e6d116","5c40340d3df24927b773c15c3887a3f2","05e24b9d27004cdfb864e2e0c683b481","03a434e2820c4044aebc1957f4bb0297","d049d28b033040ce93ab73c200d46c5d","d3b32b98f26f4dc58b6e829bdc1eb055","d19e64c91118402a93f4fa9232d3ea5c","2768976a780348cda5a46ee47b07f85c","8dd8547729654ee0b2fbbec228ea21a0","c534e3235c0043b6a5a9593c3df7eb30","a39e8258764144f7810d7d3e0e312073","be0c6ace078541dba8ac08d27cc8f325","fcf3dd1d471c481ea9c9475e426e7a47","f60dee0661c24ecb9b17ab3b77c6f139","4923e1e54fbe4f569fc73238533bfe99","6e3fe0f9c38b482bb930ecee7b10500e","b9bd78f28e4d468f84e5c2c0c9ef8f15","8a7ffa643aff4324857604f7665ed22f","99ffc3bb3f3e4017af96e7e432db6640","1440e0c23fd044369a7a007c22af1d72","02d832729f7f4c37949894f6bae76859","618e2b01156849b3904f8b84321e84e9","a8ab17f45f454278847427f69d30fd22","41fd7baa35cd4aa9b7157cd32be0af48","bd19a9ba98444ae7a39cf891459615a1","92f7d9f8a4b644518d9ec4db58b3acd2","69a99d6d0309497f8161168403025435","464d0e1b77184c2a9ea4e947fc8bbbd1","450654971f8c45bba220ba722ceb72ff","92943ee30fed451fb72067e4a1b03d75","6b6e4122088b41458556a2e48209714d","57895b44194a46d0a09c91dc22bb4e47","622cc89c603b42c78a4a7f3ad6ac7a9d","25088964a3544be7ad141dfbcb90d9d3"]},"executionInfo":{"elapsed":81712,"status":"ok","timestamp":1696001040994,"user":{"displayName":"Ahmed Hany","userId":"11066611178035959836"},"user_tz":-180},"id":"xGdZtnZI3K6m","outputId":"4269a7d9-0c55-4edb-d5f1-e7d8854969ae"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c2d3669a5994af09b205dafaee9aecf","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc23c68e5a2449af9da6af816694ebb2","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9313de894468429da6b9bca9e19db9af","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6deda7d375ac4ee689cc26ce9fbb67d4","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45295bce67c141d4b40cec6d07a0890c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/99357 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c40340d3df24927b773c15c3887a3f2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24840 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcf3dd1d471c481ea9c9475e426e7a47","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/99357 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41fd7baa35cd4aa9b7157cd32be0af48","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/24840 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","# Load previously saved datasets (training and validation) from the specified paths\n","dataset = load_dataset('text', data_files={'train': train_path, 'validation': val_path})\n","\n","# Setting the padding token for the tokenizer (GPT-2 uses the EOS token as padding)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Define a function to tokenize data:\n","# - `padding='max_length'` ensures all tokenized outputs have the same length.\n","# - `truncation=True` ensures that examples longer than 512 tokens are truncated to that number.\n","# - `return_tensors='pt'` makes sure the tokenized outputs are returned as PyTorch tensors.\n","def tokenize_function(examples):\n","    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n","\n","# Apply the tokenization function to the entire dataset (both training and validation)\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","\n","# Save the tokenized datasets to disk for easier retrieval in future sessions\n","tokenized_datasets.save_to_disk(f\"{base_dir}/tokenized_datasets\")"]}]}